{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1bac38b",
   "metadata": {},
   "source": [
    "# Email Spam Detection Model - Troubleshooting\n",
    "\n",
    "This notebook contains code to analyze and fix the stratification error in the train_test_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe5af52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset file exists: emails.csv\n",
      "File size: 8989491 bytes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_curve\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Check if the dataset file exists\n",
    "dataset_path = \"emails.csv\"\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset file exists: {dataset_path}\")\n",
    "    print(f\"File size: {os.path.getsize(dataset_path)} bytes\")\n",
    "else:\n",
    "    print(f\"Dataset file does not exist: {dataset_path}\")\n",
    "    # Check if there's an alternative file\n",
    "    alt_path = \"emails_old.csv\"\n",
    "    if os.path.exists(alt_path):\n",
    "        print(f\"Alternative dataset file exists: {alt_path}\")\n",
    "        print(f\"File size: {os.path.getsize(alt_path)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "162c65d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (6219, 2)\n",
      "\n",
      "Value counts of spam labels:\n",
      "spam\n",
      "0    4443\n",
      "1    1776\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage of spam: 28.56%\n",
      "\n",
      "Number of duplicates: 49\n",
      "After removing duplicates: (6170, 2)\n",
      "\n",
      "Missing values:\n",
      "text    0\n",
      "spam    0\n",
      "dtype: int64\n",
      "After removing missing values: (6170, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"emails.csv\", encoding='ISO-8859-1')\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "\n",
    "# Basic data exploration\n",
    "print(\"\\nValue counts of spam labels:\")\n",
    "print(data['spam'].value_counts())\n",
    "print(f\"\\nPercentage of spam: {data['spam'].mean()*100:.2f}%\")\n",
    "\n",
    "# Check for and handle duplicates\n",
    "duplicates = data.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicates: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    print(f\"After removing duplicates: {data.shape}\")\n",
    "\n",
    "# Check for and handle missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(data.isnull().sum())\n",
    "data = data.dropna(subset=['text'])  # Drop rows with missing text\n",
    "print(f\"After removing missing values: {data.shape}\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x=data['spam'])\n",
    "plt.title('Spam vs Non-Spam Distribution')\n",
    "plt.savefig('class_distribution.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee92df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define better text preprocessing\n",
    "def enhanced_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    More sophisticated text cleaning function that preserves some \n",
    "    potentially useful spam indicators\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        \n",
    "        # Replace URLs with 'urlplaceholder'\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+', ' urlplaceholder ', text)\n",
    "        \n",
    "        # Replace currency symbols with 'moneysymbol'\n",
    "        text = re.sub(r'[$€£¥]', ' moneysymbol ', text)\n",
    "        \n",
    "        # Replace email addresses with 'emailaddr'\n",
    "        text = re.sub(r'\\S+@\\S+', ' emailaddr ', text)\n",
    "        \n",
    "        # Replace phone numbers with 'phonenumber'\n",
    "        text = re.sub(r'\\b(?:\\d{3}[-.]?){2}\\d{4}\\b', ' phonenumber ', text)\n",
    "        \n",
    "        # Replace numbers with 'numbr'\n",
    "        text = re.sub(r'\\d+', ' numbr ', text)\n",
    "        \n",
    "        # Replace multiple exclamation/question marks with special tokens\n",
    "        text = re.sub(r'!!+', ' multiexclaim ', text)\n",
    "        text = re.sub(r'\\?\\?+', ' multiquestion ', text)\n",
    "        \n",
    "        # Replace remaining non-alphanumeric characters with space\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1286ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply enhanced text cleaning\n",
    "X = data['text'].apply(enhanced_text_cleaning)\n",
    "y = data['spam'].values\n",
    "\n",
    "# Feature engineering: Add additional features\n",
    "def extract_text_features(df):\n",
    "    \"\"\"Extract additional features from text\"\"\"\n",
    "    if isinstance(df, pd.Series):\n",
    "        series = df\n",
    "    else:\n",
    "        series = df.iloc[:, 0]  # Assuming the text is in the first column\n",
    "        \n",
    "    # Initialize empty DataFrame\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Text length\n",
    "    features['text_length'] = series.apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    \n",
    "    # Count of specific patterns\n",
    "    features['exclamation_count'] = series.apply(lambda x: x.count('!') if isinstance(x, str) else 0)\n",
    "    features['question_count'] = series.apply(lambda x: x.count('?') if isinstance(x, str) else 0)\n",
    "    features['uppercase_ratio'] = series.apply(\n",
    "        lambda x: sum(1 for c in x if c.isupper()) / max(len(x), 1) if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Presence of special tokens (from our preprocessing)\n",
    "    features['has_url'] = series.apply(lambda x: 1 if 'urlplaceholder' in x else 0 if isinstance(x, str) else 0)\n",
    "    features['has_money'] = series.apply(lambda x: 1 if 'moneysymbol' in x else 0 if isinstance(x, str) else 0)\n",
    "    features['has_email'] = series.apply(lambda x: 1 if 'emailaddr' in x else 0 if isinstance(x, str) else 0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Create text features\n",
    "text_features = extract_text_features(data['text'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaecae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum class count: 1765\n",
      "Used stratified sampling\n",
      "\n",
      "Training data shape: (4936,)\n",
      "Testing data shape: (1234,)\n",
      "\n",
      "Training set class distribution:\n",
      "[3524 1412]\n",
      "Test set class distribution:\n",
      "[881 353]\n"
     ]
    }
   ],
   "source": [
    "# Check if stratification is possible\n",
    "class_counts = np.bincount(y)\n",
    "min_class_count = np.min(class_counts[class_counts > 0])\n",
    "print(f\"\\nMinimum class count: {min_class_count}\")\n",
    "\n",
    "# Train-test split with stratification if possible, otherwise without\n",
    "if min_class_count >= 2:\n",
    "    X_train, X_test, y_train, y_test, features_train, features_test = train_test_split(\n",
    "        X, y, text_features, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    print(\"Used stratified sampling\")\n",
    "else:\n",
    "    # If stratification is not possible, use regular train-test split\n",
    "    X_train, X_test, y_train, y_test, features_train, features_test = train_test_split(\n",
    "        X, y, text_features, test_size=0.2, random_state=42\n",
    "    )\n",
    "    print(\"Used regular sampling (stratification not possible)\")\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in training and test sets\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(np.bincount(y_train))\n",
    "print(\"Test set class distribution:\")\n",
    "print(np.bincount(y_test))\n",
    "\n",
    "# Create classifiers to compare\n",
    "classifiers = {\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, C=1.0),\n",
    "    'Linear SVM': LinearSVC(C=1.0, dual=False),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b5d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model and return metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    confusion = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'report': report,\n",
    "        'confusion_matrix': confusion\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c025fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TF-IDF Approach ---\n",
      "\n",
      "Training Multinomial Naive Bayes...\n",
      "Multinomial Naive Bayes Accuracy: 0.9789\n",
      "Precision (Spam): 0.9767\n",
      "Recall (Spam): 0.9490\n",
      "F1-Score (Spam): 0.9626\n",
      "\n",
      "Training Logistic Regression...\n",
      "Multinomial Naive Bayes Accuracy: 0.9789\n",
      "Precision (Spam): 0.9767\n",
      "Recall (Spam): 0.9490\n",
      "F1-Score (Spam): 0.9626\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.9814\n",
      "Precision (Spam): 0.9769\n",
      "Recall (Spam): 0.9575\n",
      "F1-Score (Spam): 0.9671\n",
      "\n",
      "Training Linear SVM...\n",
      "Logistic Regression Accuracy: 0.9814\n",
      "Precision (Spam): 0.9769\n",
      "Recall (Spam): 0.9575\n",
      "F1-Score (Spam): 0.9671\n",
      "\n",
      "Training Linear SVM...\n",
      "Linear SVM Accuracy: 0.9862\n",
      "Precision (Spam): 0.9746\n",
      "Recall (Spam): 0.9773\n",
      "F1-Score (Spam): 0.9760\n",
      "\n",
      "Training Random Forest...\n",
      "Linear SVM Accuracy: 0.9862\n",
      "Precision (Spam): 0.9746\n",
      "Recall (Spam): 0.9773\n",
      "F1-Score (Spam): 0.9760\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Accuracy: 0.9182\n",
      "Precision (Spam): 0.9809\n",
      "Recall (Spam): 0.7280\n",
      "F1-Score (Spam): 0.8358\n",
      "Random Forest Accuracy: 0.9182\n",
      "Precision (Spam): 0.9809\n",
      "Recall (Spam): 0.7280\n",
      "F1-Score (Spam): 0.8358\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF approach\n",
    "print(\"\\n--- TF-IDF Approach ---\")\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Create a pipeline with TF-IDF\n",
    "    tfidf_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=10000,\n",
    "            min_df=2\n",
    "        )),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Train and evaluate\n",
    "    tfidf_pipeline.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(tfidf_pipeline, X_test, y_test)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"{name} Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision (Spam): {metrics['report']['1']['precision']:.4f}\")\n",
    "    print(f\"Recall (Spam): {metrics['report']['1']['recall']:.4f}\")\n",
    "    print(f\"F1-Score (Spam): {metrics['report']['1']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b4d0667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model (we'll choose TF-IDF with the classifier that performed best)\n",
    "# Assuming Logistic Regression was the best based on common performance patterns\n",
    "best_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        max_features=10000,\n",
    "        min_df=2\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, C=1.0))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92f4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Hyperparameter Tuning ---\n",
      "Best parameters: {'classifier__C': 10.0, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n",
      "Best parameters: {'classifier__C': 10.0, 'tfidf__max_features': 10000, 'tfidf__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune hyperparameters\n",
    "print(\"\\n--- Hyperparameter Tuning ---\")\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__max_features': [5000, 10000],\n",
    "    'classifier__C': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(best_pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05642857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Evaluation ---\n",
      "Final Accuracy: 0.9870\n",
      "\n",
      "Classification Report:\n",
      "Not Spam:\n",
      "  Precision: 0.9898\n",
      "  Recall: 0.9921\n",
      "  F1-Score: 0.9909\n",
      "Spam:\n",
      "  Precision: 0.9801\n",
      "  Recall: 0.9745\n",
      "  F1-Score: 0.9773\n",
      "Final Accuracy: 0.9870\n",
      "\n",
      "Classification Report:\n",
      "Not Spam:\n",
      "  Precision: 0.9898\n",
      "  Recall: 0.9921\n",
      "  F1-Score: 0.9909\n",
      "Spam:\n",
      "  Precision: 0.9801\n",
      "  Recall: 0.9745\n",
      "  F1-Score: 0.9773\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation\n",
    "print(\"\\n--- Final Model Evaluation ---\")\n",
    "final_metrics = evaluate_model(best_model, X_test, y_test)\n",
    "\n",
    "print(f\"Final Accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "for label, metrics in final_metrics['report'].items():\n",
    "    if label in ['0', '1']:\n",
    "        class_name = 'Spam' if label == '1' else 'Not Spam'\n",
    "        print(f\"{class_name}:\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score: {metrics['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd75d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = final_metrics['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Spam', 'Spam'],\n",
    "            yticklabels=['Not Spam', 'Spam'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75c69bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prediction function\n",
    "def predict_spam(model, texts):\n",
    "    predictions = model.predict(texts)\n",
    "    probabilities = None\n",
    "    \n",
    "    # Get probabilities if the model supports predict_proba\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        probabilities = model.predict_proba(texts)[:, 1]\n",
    "    \n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        result = {\n",
    "            'text': text,\n",
    "            'prediction': 'Spam' if predictions[i] == 1 else 'Not Spam'\n",
    "        }\n",
    "        if probabilities is not None:\n",
    "            result['spam_probability'] = probabilities[i]\n",
    "        results.append(result)\n",
    "    \n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85d8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable distribution:\n",
      "(array([0, 1]), array([4405, 1765]))\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "test_emails = [\n",
    "    'Hey i am Elon Musk. Get a brand new car from Tesla',\n",
    "    'Hi Mary, just checking in about our meeting tomorrow at 2pm. See you then!',\n",
    "    'CONGRATULATIONS! You have won a FREE iPhone! Click here to claim now!!!',\n",
    "    'The quarterly report is attached. Please review before the board meeting.'\n",
    "]\n",
    "\n",
    "# Check distribution in target variable\n",
    "print(\"Target variable distribution:\")\n",
    "print(np.unique(y, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1de15b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example Predictions ---\n",
      "Text: Hey i am Elon Musk. Get a brand new car from Tesla\n",
      "Prediction: Spam\n",
      "Spam probability: 0.7105\n",
      "\n",
      "Text: Hi Mary, just checking in about our meeting tomorr...\n",
      "Prediction: Not Spam\n",
      "Spam probability: 0.0781\n",
      "\n",
      "Text: CONGRATULATIONS! You have won a FREE iPhone! Click...\n",
      "Prediction: Spam\n",
      "Spam probability: 0.9818\n",
      "\n",
      "Text: The quarterly report is attached. Please review be...\n",
      "Prediction: Not Spam\n",
      "Spam probability: 0.0234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n--- Example Predictions ---\")\n",
    "predictions = predict_spam(best_model, test_emails)\n",
    "for pred in predictions:\n",
    "    print(f\"Text: {pred['text'][:50]}...\" if len(pred['text']) > 50 else f\"Text: {pred['text']}\")\n",
    "    print(f\"Prediction: {pred['prediction']}\")\n",
    "    if 'spam_probability' in pred:\n",
    "        print(f\"Spam probability: {pred['spam_probability']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86b7966e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as spam_detection_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_filename = \"spam_detection_model.pkl\"\n",
    "pickle.dump(best_model, open(model_filename, 'wb'))\n",
    "print(f\"Model saved as {model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
